# LLM Configuration
# Provider options: ollama, gemini, openai, siliconflow
LLM_PROVIDER=ollama

# Translation Mode
# real: Perform actual translation via LLM
# mock: Return dummy data for testing
TRANSLATE_LLM_MODE=real

# Server Configuration
PORT=5001
HOST=0.0.0.0

# Ollama Configuration
# Docker 環境使用：http://ollama:11434
# 本地開發使用：http://localhost:11434
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_TIMEOUT=180
# OLLAMA_NUM_GPU=1
# OLLAMA_NUM_GPU_LAYERS=35
# OLLAMA_NUM_CTX=4096
# OLLAMA_NUM_THREAD=8
# OLLAMA_FORCE_GPU=false

# Gemini Configuration
# Get key from https://aistudio.google.com/
GEMINI_API_KEY=AIzaSy...
GEMINI_MODEL=gemini-1.5-flash
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
GEMINI_TIMEOUT=180

# OpenAI Configuration
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_TIMEOUT=60

# SiliconFlow Configuration (OpenAI Compatible)
# SILICONFLOW_API_KEY=sk-xxxxxxxx
# SILICONFLOW_MODEL=Qwen/Qwen2.5-7B-Instruct
# SILICONFLOW_BASE_URL=https://api.siliconflow.cn/v1

# Translation Settings
SOURCE_LANGUAGE=auto
# Context strategy: none, slide, prev_next (experimental)
LLM_CONTEXT_STRATEGY=none
# Path to glossary JSON file (optional)
LLM_GLOSSARY_PATH=
# Handling errors: 0=Stop, 1=Fallback to original text
LLM_FALLBACK_ON_ERROR=0

# Performance / Rate Limiting
# Process chunks one by one (1) or valid parallel logic if implemented (0)
LLM_SINGLE_REQUEST=1
LLM_CHUNK_SIZE=40
LLM_MAX_RETRIES=2
LLM_RETRY_BACKOFF=0.8
LLM_RETRY_MAX_BACKOFF=8
LLM_CHUNK_DELAY=0

# PDF
PDF_OCR_DPI=300
PDF_OCR_LANG=chi_tra+vie+eng
PDF_OCR_CONF_MIN=15