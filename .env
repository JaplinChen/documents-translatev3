# LLM Configuration
# Provider options: ollama, gemini, openai, siliconflow
LLM_PROVIDER=ollama

# Translation Mode
# real: Perform actual translation via LLM
# mock: Return dummy data for testing
TRANSLATE_LLM_MODE=real

# Server Configuration
PORT=5002
HOST=0.0.0.0

# Ollama Configuration
# Docker 環境使用：http://ollama:11434
# 本地開發使用：http://localhost:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_TIMEOUT=180

# Translation Settings
SOURCE_LANGUAGE=auto
LLM_CONTEXT_STRATEGY=none
LLM_FALLBACK_ON_ERROR=0

# Performance / Rate Limiting
LLM_SINGLE_REQUEST=1
LLM_CHUNK_SIZE=10
LLM_MAX_RETRIES=2
LLM_RETRY_BACKOFF=0.8
LLM_RETRY_MAX_BACKOFF=8
LLM_CHUNK_DELAY=0

# PDF OCR Settings
PDF_OCR_DPI=200
PDF_OCR_LANG=auto
PDF_OCR_CONF_MIN=10
PDF_OCR_PSM=6
PDF_OCR_ENGINE=paddle
PDF_OCR_ALLOW_PADDLE=1
PDF_OCR_PADDLE_FALLBACK=1
IMAGE_OCR_PREFER_PADDLE=1
IMAGE_OCR_ENGINE=paddle
PADDLE_OCR_USE_ANGLE=1
IMAGE_OCR_LANG=auto
